{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers==v4.39.2 in /opt/conda/envs/imp/lib/python3.10/site-packages (4.39.2)\n","Requirement already satisfied: filelock in /opt/conda/envs/imp/lib/python3.10/site-packages (from transformers==v4.39.2) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/envs/imp/lib/python3.10/site-packages (from transformers==v4.39.2) (0.23.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/imp/lib/python3.10/site-packages (from transformers==v4.39.2) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/imp/lib/python3.10/site-packages (from transformers==v4.39.2) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/imp/lib/python3.10/site-packages (from transformers==v4.39.2) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/imp/lib/python3.10/site-packages (from transformers==v4.39.2) (2024.5.15)\n","Requirement already satisfied: requests in /opt/conda/envs/imp/lib/python3.10/site-packages (from transformers==v4.39.2) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/imp/lib/python3.10/site-packages (from transformers==v4.39.2) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/imp/lib/python3.10/site-packages (from transformers==v4.39.2) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/imp/lib/python3.10/site-packages (from transformers==v4.39.2) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/imp/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==v4.39.2) (2024.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/imp/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==v4.39.2) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/imp/lib/python3.10/site-packages (from requests->transformers==v4.39.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/imp/lib/python3.10/site-packages (from requests->transformers==v4.39.2) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/imp/lib/python3.10/site-packages (from requests->transformers==v4.39.2) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/imp/lib/python3.10/site-packages (from requests->transformers==v4.39.2) (2024.2.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# !pip install transformers==v4.39.2\n","# !pip uninstall -y transformers\n","# !pip install git+https://github.com/huggingface/transformers\n","# !pip install -q pillow accelerate einops\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Name: transformers\n","Version: 4.39.2\n","Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n","Home-page: https://github.com/huggingface/transformers\n","Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n","Author-email: transformers@huggingface.co\n","License: Apache 2.0 License\n","Location: /opt/conda/envs/imp/lib/python3.10/site-packages\n","Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n","Required-by: peft\n"]}],"source":["!pip show transformers"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/imp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"data":{"text/plain":["'2.0.1+cu117'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from PIL import Image\n","# torch.set_default_device(\"cuda\")\n","default_device = torch.device(\"cuda:0\")\n","torch.cuda.set_device(default_device)\n","torch.__version__\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# !git clone https://huggingface.co/MILVLG/imp-v1-3b"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/imp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[2024-05-20 14:45:05,422] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/imp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Downloading shards: 100%|██████████| 7/7 [12:46<00:00, 109.45s/it]\n","Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.58it/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["#Create model\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"MILVLG/imp-v1-3b\", \n","    torch_dtype=torch.float16, \n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    # force_download=True,\n","    resume_download=True)\n","tokenizer = AutoTokenizer.from_pretrained(\"MILVLG/imp-v1-3b\", trust_remote_code=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","\n","# for detection\n","text_car = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \\\n","        USER: <image>\\nIs there any cars,persons,traffic lights,road signals, motorcycles in the image, \\\n","            give me their bounding box coordinates in the image, just these 5 classes, each object must has 4 coordinates, \\\n","            if there is no this kand of object, just return None? \\\n","            Give me the answer in this format : \\\n","                {'cars': [[x_min,y_min, x_max,y_max], [x_min,y_min,x_max,y_max], ..., [x_min,y_min,x_max,y_max]], \\\n","                'persons': [[x_min,y_min, x_max,y_max], [x_min,y_min,x_max,y_max], ..., [x_min,y_min,x_max,y_max]], \\\n","                    'traffic lights': [[x_min,y_min, x_max,y_max], [x_min,y_min,x_max,y_max], ..., [x_min,y_min,x_max,y_max]], \\\n","                        'road signals': [[x_min,y_min, x_max,y_max], [x_min,y_min,x_max,y_max], ..., [x_min,y_min,x_max,y_max]], \\\n","                            'motorcycles': [[x_min,y_min, x_max,y_max], [x_min,y_min,x_max,y_max], ..., [x_min,y_min,x_max,y_max]],} ASSISTANT:\"\n","\n","text1 = \"\"\"\n","Imagine a friendly chat between an inquisitive user and an AI assistant, where the assistant is eager to provide helpful and detailed responses to the user's questions.\n","\n","USER: [Attaches an image]\n","Could you please assist me in identifying any cars, people, traffic lights, road signals, or motorcycles in this image? Just these 5 classes, If any of these objects are present, could you kindly provide their bounding box coordinates within the image? For each of these five categories, please include four coordinates representing the minimum and maximum values for both the x and y axes. If any category of object is not detected, a placeholder like [] should be given.\n","\n","The desired response format is as follows:\n","{\n","    'cars': [[x_min, y_min, x_max, y_max], ...],\n","    'persons': [[x_min, y_min, x_max, y_max], ...],\n","    'traffic lights': [[x_min, y_min, x_max, y_max], ...],\n","    'road signals': [[x_min, y_min, x_max, y_max], ...],\n","    'motorcycles': [[x_min, y_min, x_max, y_max], ...]\n","} \n","\n","ASSISTANT:\n","\n","\"\"\"\n","\n","text2 = \"\"\"\n","Scene: A friendly interaction between an inquisitive user and an AI assistant who specializes in image analysis.\n","\n","User: I've got an image here <image>, could you tell me if there are any cars, people, traffic lights, road signs, or motorcycles in it? \\\n","    If there are, I need the bounding box coordinates for each, defined by four points. Just stick to these five categories, please. \\\n","        Oh, and if there's nothing of that sort in the picture, just let me know by saying 'None'. I'd appreciate it if you could format your response like this:\n","\n","```json\n","{\n","  \"cars\": [\n","    [x_min, y_min, x_max, y_max],\n","    [x_min, y_min, x_max, y_max],\n","    ...\n","  ],\n","  \"persons\": [\n","    [x_min, y_min, x_max, y_max],\n","    ...\n","  ],\n","  \"traffic_lights\": [\n","    ...\n","  ],\n","  \"road_signals\": [\n","    ...\n","  ],\n","  \"motorcycles\": [\n","    ...\n","  ]\n","}\n","```\n","\"\"\"\n","\n","text_persons = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \\\n","        USER: <image>\\nIs there any persons in the image, give me their bounding box coordinates in the image, each person must has 4 cordinates, if there is no person, just return None? \\\n","            Give me the answer in this format : {'person1': [x_min,y_min, x_max,y_max], 'person2': [x_min,y_min,x_max,y_max], ...} ASSISTANT:\"\n","text_traffic_lights = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \\\n","        USER: <image>\\nIs there any traffic lights in the image, give me their bounding box coordinates in the image, each traffic light must has 4 cordinates, if there is no traffic light, just return None? \\\n","            Give me the answer in this format : {'traffic_light1': [x_min,y_min, x_max,y_max], 'traffic_light2': [x_min,y_min,x_max,y_max], ...} ASSISTANT:\"\n","text_road_signals = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \\\n","        USER: <image>\\nIs there any road signals in the image, give me their bounding box coordinates in the image, each road signal must has 4 cordinates, if there is no road signal, just return None? \\\n","            Give me the answer in this format : {'road_signal1': [x_min,y_min, x_max,y_max], 'road_signal2': [x_min,y_min,x_max,y_max], ...} ASSISTANT:\"\n","text_motorcycles = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \\\n","        USER: <image>\\nIs there any motorcycles in the image, give me their bounding box coordinates in the image, each motorcycle must has 4 cordinates, if there is no motorcycle, just return None? \\\n","            Give me the answer in this format : {'motorcycle1': [x_min,y_min, x_max,y_max], 'motorcycle2': [x_min,y_min,x_max,y_max], ...} ASSISTANT:\"\n","import json\n","from tqdm import tqdm\n","\n","img_path = \"/home/data/images\"\n","img_lst = os.listdir(img_path)\n","img_lst = sorted(img_lst, key=lambda x: int(x.split('.')[0]))\n","out_dir = img_path.replace(\"images\", \"bboxes\")\n","os.makedirs(out_dir, exist_ok=True)\n","for e in tqdm(img_lst):\n","    bbox = {}\n","    print(f\"processing {e}\")\n","    file_pth = os.path.join(img_path, e)\n","    image0 = Image.open(file_pth) # 000460\n","    input_ids = tokenizer(text1, return_tensors='pt').input_ids.to(default_device)\n","    image_tensor0 = model.image_preprocess(image0).to(default_device)\n","\n","    #Generate the answer\n","    output_ids = model.generate(\n","        input_ids,\n","        max_new_tokens=500,\n","        images=image_tensor0,\n","        use_cache=True)[0]\n","    car_pos = tokenizer.decode(output_ids[input_ids.shape[1]:], skip_special_tokens=True).strip()\n","    try:\n","      bbox[e] = eval(car_pos)\n","    except:\n","       import pdb;pdb.set_trace()\n","    with open(os.path.join(out_dir, e.replace(\".jpg\",\".json\")),\"w\") as json_file:\n","        json.dump(bbox, json_file, indent=4)\n","    # print(car_pos)\n","# print(bbox)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'bbox' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(cars_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cars_dir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcar.json\u001b[39m\u001b[38;5;124m\"\u001b[39m),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m----> 5\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[43mbbox\u001b[49m, json_file, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'bbox' is not defined"]}],"source":["import json\n","json_dir = \"/home/data/bboxes\"\n","js_list = os.listdir(json_dir)\n","with open(js_list[0],\"r\") as json_file:\n","    jf = json.load(json_file)\n","\n","obj_bbox = jf[js_list[0].replace(\".josn\", \".jpg\")]\n","\n","for k,v in obj_bbox.items():\n","    "]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"float() argument must be a string or a real number, not 'list'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m car_pos_wh \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m car \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28meval\u001b[39m(car_pos)\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m----> 5\u001b[0m     car_pos_wh\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28mfloat\u001b[39m(item) \u001b[38;5;241m*\u001b[39m w \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(item) \u001b[38;5;241m*\u001b[39m h \u001b[38;5;28;01mfor\u001b[39;00m idx, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(car)])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(car_pos_wh)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n","Cell \u001b[0;32mIn[25], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m car_pos_wh \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m car \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28meval\u001b[39m(car_pos)\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m----> 5\u001b[0m     car_pos_wh\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m w \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(item) \u001b[38;5;241m*\u001b[39m h \u001b[38;5;28;01mfor\u001b[39;00m idx, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(car)])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(car_pos_wh)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'list'"]}],"source":["# car_pos = [[0.34, 0.38, 0.64, 0.82], [0.68, 0.51, 1.0, 0.74],[0.68, 0.51, 1.0, 0.74]]\n","h, w = 1028, 1912\n","car_pos_wh = []\n","for car in eval(car_pos).values():\n","    car_pos_wh.append([float(item) * w if idx % 2 == 0 else float(item) * h for idx, item in enumerate(car)])\n","print(car_pos_wh)\n","\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# image = np.array(image0)\n","# Convert RGB to BGR (OpenCV uses BGR format)\n","# image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","image = cv2.imread(file)\n","for point in car_pos_wh:\n","    print(point)\n","    # Draw the box on the image\n","    color = (0, 255, 0)  # Green color in BGR\n","    thickness = 2  # Line thickness\n","    cv2.rectangle(image, (int(point[0]), int(point[1])), (int(point[2]), int(point[3])), color, thickness)\n","\n","\n","# Display the image with bounding boxes\n","plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for Matplotlib\n","plt.axis('off')  # Hide axes\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Yes, there is a car, a motorcycle, a person, traffic lights, and road signals in the image. for attacks\n","# Yes, there is a car driving down the street, a traffic light, and a road signal in the image. for ori"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inputs_embeds = model.get_input_embeddings()(input_ids)\n","inputs_embeds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["img_f = model.encode_images(image_tensor.to(torch.float16))\n","# img_f.shape # torch.Size([1, 729, 2560])\n","model.en"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs = model(\n","    input_ids=input_ids,\n","    inputs_embeds=inputs_embeds,\n","    images=image\n",")\n","\n","# Extract the relevant features and embeddings from the outputs\n","text_features = outputs.hidden_states\n","# outputs.logits.shape # torch.Size([1, 49, 51200])\n","text_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s = model.generate(\n","    input_ids,\n","    max_new_tokens=100,\n","    images=image_tensor,\n","    use_cache=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":2}
